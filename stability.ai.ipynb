{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768f0b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stability-sdk\n",
      "  Downloading stability_sdk-0.3.1-py3-none-any.whl (47 kB)\n",
      "     ---------------------------------------- 47.8/47.8 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Pillow in c:\\users\\neera\\anaconda3\\lib\\site-packages (from stability-sdk) (8.4.0)\n",
      "Collecting grpcio==1.48.1\n",
      "  Downloading grpcio-1.48.1-cp39-cp39-win_amd64.whl (3.6 MB)\n",
      "     ---------------------------------------- 3.6/3.6 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting grpcio-tools==1.48.1\n",
      "  Downloading grpcio_tools-1.48.1-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting protobuf==3.19.5\n",
      "  Downloading protobuf-3.19.5-cp39-cp39-win_amd64.whl (895 kB)\n",
      "     -------------------------------------- 895.9/895.9 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\neera\\anaconda3\\lib\\site-packages (from grpcio==1.48.1->stability-sdk) (1.16.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\neera\\anaconda3\\lib\\site-packages (from grpcio-tools==1.48.1->stability-sdk) (58.0.4)\n",
      "Installing collected packages: python-dotenv, protobuf, grpcio, grpcio-tools, stability-sdk\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.4\n",
      "    Uninstalling protobuf-3.19.4:\n",
      "      Successfully uninstalled protobuf-3.19.4\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.44.0\n",
      "    Uninstalling grpcio-1.44.0:\n",
      "      Successfully uninstalled grpcio-1.44.0\n",
      "Successfully installed grpcio-1.48.1 grpcio-tools-1.48.1 protobuf-3.19.5 python-dotenv-0.21.0 stability-sdk-0.3.1\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stability-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5953ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "from PIL import Image\n",
    "from stability_sdk import client\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "\n",
    "# Our Host URL should not be prepended with \"https\" nor should it have a trailing slash.\n",
    "os.environ['STABILITY_HOST'] = 'grpc.stability.ai:443'\n",
    "\n",
    "# Sign up for an account at the following link to get an API Key.\n",
    "# https://beta.dreamstudio.ai/membership\n",
    "\n",
    "# Click on the following link once you have created an account to be taken to your API Key.\n",
    "# https://beta.dreamstudio.ai/membership?tab=apiKeys\n",
    "\n",
    "# Paste your API Key below.\n",
    "\n",
    "os.environ['STABILITY_KEY'] = 'sk-Jev2xqvCIPopHknhgnC7YTAEnnq2St6ysbAwN7FLPODtUjY2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2bd29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our connection to the API.\n",
    "stability_api = client.StabilityInference(\n",
    "    key=os.environ['STABILITY_KEY'], # API Key reference.\n",
    "    verbose=True, # Print debug messages.\n",
    "    engine=\"stable-diffusion-v1-5\", # Set the engine to use for generation.\n",
    "    # Available engines: stable-diffusion-v1 stable-diffusion-v1-5 stable-diffusion-512-v2-0 stable-diffusion-768-v2-0 \n",
    "    # stable-diffusion-512-v2-1 stable-diffusion-768-v2-1 stable-inpainting-v1-0 stable-inpainting-512-v2-0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a754569",
   "metadata": {},
   "source": [
    "## TEXT-TO-IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a3bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our initial generation parameters.\n",
    "answers = stability_api.generate(\n",
    "    prompt=\"The satellite hovers high above the Earth, its powerful cameras capturing a stunning view of our planet. The blue of the oceans contrasts with the green and brown of the land masses, creating a beautiful mosaic of colors.\",\n",
    "    seed=1, # If a seed is provided, the resulting generated image will be deterministic.\n",
    "                    # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                    # Note: This isn't quite the case for Clip Guided generations, which we'll tackle in a future example notebook.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30. \n",
    "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    samples=1, # Number of images to generate, defaults to 1 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, save generated images.\n",
    "for resp in answers:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            img = Image.open(io.BytesIO(artifact.binary))\n",
    "            img.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501df8de",
   "metadata": {},
   "source": [
    "## IMAGE-TO-IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88372267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our initial generation parameters.\n",
    "answers = stability_api.generate(\n",
    "    prompt=\"The satellite hovers high above the Earth, its powerful cameras capturing a stunning view of our planet. The blue of the oceans contrasts with the green and brown of the land masses, creating a beautiful mosaic of colors.\",\n",
    "    seed=2, # If a seed is provided, the resulting generated image will be deterministic.\n",
    "                    # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                    # Note: This isn't quite the case for Clip Guided generations, which we'll tackle in a future example notebook.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30. \n",
    "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, display generated image.\n",
    "for resp in answers:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            global img\n",
    "            img = Image.open(io.BytesIO(artifact.binary))\n",
    "            img.save(str(artifact.seed)+ \".png\") # Save our generated images its seed number as the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97036803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our initial generation parameters.\n",
    "answers2 = stability_api.generate(\n",
    "    prompt=\"sketchy crayon outline on old paper rolling greens with blue daisies and weeping willow trees under a blue alien sky\",\n",
    "    init_image=img, # Assign our previously generated img as our Initial Image for transformation.\n",
    "    start_schedule=0.6, # Set the strength of our prompt in relation to our initial image.\n",
    "    seed=3, # If attempting to transform an image that was previously generated with our API,\n",
    "                    # initial images benefit from having their own distinct seed rather than using the seed of the original image generation.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30. \n",
    "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, display generated image.\n",
    "for resp in answers2:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            global img2\n",
    "            img2 = Image.open(io.BytesIO(artifact.binary))\n",
    "            img2.save(str(artifact.seed)+ \"-img2img.png\") # Save our generated image with its seed number as the filename and the img2img suffix so that we know this is our transformed image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f549f9",
   "metadata": {},
   "source": [
    "## INPAINTING+MASKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58f6608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.  Downloading torchvision-0.14.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 5.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\neera\\anaconda3\\lib\\site-packages (from torchvision) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\neera\\anaconda3\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Collecting torch==1.13.1\n",
      "  Downloading torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
      "     -------------------------------------- 162.5/162.5 MB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\neera\\anaconda3\\lib\\site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\neera\\anaconda3\\lib\\site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\neera\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\neera\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\neera\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\neera\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.13.1 torchvision-0.14.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a53b10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "from PIL import Image\n",
    "from stability_sdk import client\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "import numpy as np\n",
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "# Our host url should not be prepended with \"https\" nor should it have a trailing slash.\n",
    "os.environ['STABILITY_HOST'] = 'grpc.stability.ai:443'\n",
    "\n",
    "# Sign up for an account at the following link to get an API Key.\n",
    "# https://beta.dreamstudio.ai/membership\n",
    "\n",
    "# Click on the following link once you have created an account to be taken to your API Key.\n",
    "# https://beta.dreamstudio.ai/membership?tab=apiKeys\n",
    "\n",
    "# Paste your API Key below.\n",
    "os.environ['STABILITY_KEY'] = 'sk-Jev2xqvCIPopHknhgnC7YTAEnnq2St6ysbAwN7FLPODtUjY2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0daf3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up initial generation parameters, display image on generation, and safety warning for if the adult content classifier is tripped.\n",
    "\n",
    "answers = stability_api.generate(\n",
    "    prompt=\"houston, we are a 'go' for launch!\",\n",
    "    seed=4, # If a seed is provided, the resulting generated image will be deterministic.\n",
    "                    # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                    # Note: This isn't quite the case for Clip Guided generations, which we'll tackle in a future example notebook.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30. \n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped. If adult content classifier is not tripped, display generated image.\n",
    "for resp in answers:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            global img\n",
    "            img = Image.open(io.BytesIO(artifact.binary))\n",
    "            img.save(str(artifact.seed)+ \"-1-start.png\") # Save our generated image with its seed number as the filename and the 1-start suffix so that we know this was our origin generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5509a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers2 = stability_api.generate(\n",
    "    prompt=\"sketchy crayon drawing of a starship in space on old paper\",\n",
    "    init_image=img, # Assign our previously generated img as our Initial Image for transformation.\n",
    "    start_schedule=0.6, # Set the strength of our prompt in relation to our initial image.\n",
    "    seed=5, # If attempting to transform an image that was previously generated with our API,\n",
    "                    # initial images benefit from having their own distinct seed rather than using the seed of the original image generation.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30. \n",
    "    cfg_scale=7.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_lms if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, display generated image.\n",
    "for resp in answers2:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            global img2\n",
    "            img2 = Image.open(io.BytesIO(artifact.binary))\n",
    "            img2.save(str(artifact.seed)+ \"-2-img2img.png\") # Save our generated image with its seed number as the filename and the 2-img2img suffix so that we know this is our transformed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fb0607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2_grayscale = img2.convert('L')\n",
    "img2_a = np.array(img2_grayscale)\n",
    "\n",
    "mask = np.array(img2_grayscale)\n",
    "mask[img2_a<150] = 0  # This is the area that will get painted, will show up as grey.\n",
    "mask[img2_a>=150] = 1 # This is the protected area, will show up white. Protected areas won't be affected by our generating.\n",
    "\n",
    "strength = .2  # This controls the strength of our prompt relative to the init image.\n",
    "\n",
    "d = int(255 * (1-strength))\n",
    "mask *= 255-d # Converts our range from [0,1] to [0,255]\n",
    "mask += d\n",
    "\n",
    "mask = Image.fromarray(mask)\n",
    "mask.save(str(artifact.seed)+ \"-3-mask.png\") # Save our mask w/ the generation seed number as the filename and 3-mask suffix so that we know this is the mask we've created for this generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "983f5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = GaussianBlur(11,20)\n",
    "mask = blur(mask)\n",
    "mask.save(str(artifact.seed)+ \"-4-featheredmask.png\") # Save our mask w/ the generation seed number as the filename and 4-featheredmask suffix so that we know this is the feathered mask we've created for this generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f9972a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers3 = stability_api.generate(\n",
    "    prompt=\"rainbow galactic nebula, star-filled sky, spectral, psychedelic, masterpiece, artstation\",\n",
    "    init_image=img2,\n",
    "    mask_image=mask,\n",
    "    start_schedule=1,\n",
    "    seed=6, # If attempting to transform an image that was previously generated with our API,\n",
    "                    # initial images benefit from having their own distinct seed rather than using the seed of the original image generation.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30. \n",
    "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_lms if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, display generated image.\n",
    "for resp in answers3:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            global img3\n",
    "            img3 = Image.open(io.BytesIO(artifact.binary))\n",
    "            img3.save(str(artifact.seed)+ \"-5-completed.png\") # Save our completed image with its seed number as the filename, including the 5-completed suffix so that we know this is our final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88aab8",
   "metadata": {},
   "source": [
    "## CLIP GUIDANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf1ca765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from PIL import Image\n",
    "from stability_sdk import client\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "\n",
    "# Our Host URL should not be prepended with \"https\" nor should it have a trailing slash.\n",
    "os.environ['STABILITY_HOST'] = 'grpc.stability.ai:443'\n",
    "\n",
    "# Sign up for an account at the following link to get an API Key.\n",
    "# https://beta.dreamstudio.ai/membership\n",
    "\n",
    "# Click on the following link once you have created an account to be taken to your API Key.\n",
    "# https://beta.dreamstudio.ai/membership?tab=apiKeys\n",
    "\n",
    "# Paste your API Key below.\n",
    "\n",
    "os.environ['STABILITY_KEY'] = 'sk-Jev2xqvCIPopHknhgnC7YTAEnnq2St6ysbAwN7FLPODtUjY2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e519125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our initial generation parameters.\n",
    "answers = stability_api.generate(\n",
    "    prompt=\"alien city under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    seed=7, # If a seed is provided, the resulting generated image will be deterministic. What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                    # Note: CLIP Guided generations will attempt to stay near the original generation, however unlike non-clip guided inference, there's no way to guarantee a deterministic result, even with the same seed.\n",
    "    steps=50, # Step Count defaults to 50 if not specified here.\n",
    "    cfg_scale=7.0, # Influences how strongly your generation is guided to match your prompt. Setting this value higher increases the strength in which it tries to match your prompt. Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2S_ANCESTRAL, # Choose which sampler we want to denoise our generation with. Defaults to k_dpmpp_2s_ancestral. CLIP Guidance only supports ancestral samplers.\n",
    "                                                  # (Available Samplers: ddim, k_euler_ancestral, k_dpm_2_ancestral, k_dpmpp_2s_ancestral)\n",
    "    guidance_preset=generation.GUIDANCE_PRESET_FAST_GREEN # Enables CLIP Guidance. \n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped. If adult content classifier is not tripped, display generated image.\n",
    "for resp in answers:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            img = Image.open(io.BytesIO(artifact.binary))\n",
    "            img.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce534a",
   "metadata": {},
   "source": [
    "## MULTI-PROMPTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5e5ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = stability_api.generate(\n",
    "    prompt=\"a mountain landscape in the style of thomas kinkade\",\n",
    "    seed=8, # If a seed is provided, the resulting generated image will be deterministic.\n",
    "                    # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                    # Note: This isn't quite the case for Clip Guided generations, which we'll tackle in a future example notebook.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30. \n",
    "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    samples=1, # Number of images to generate, defaults to 1 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, save generated images.\n",
    "for resp in answers:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            img = Image.open(io.BytesIO(artifact.binary))\n",
    "            img.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3f381a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: With multi-prompting, we can mix concepts by assigning each prompt a specific weight. Concepts are combined according to their weight. \n",
    "# Prompts with token lengths beyond 77 will be truncated. Default prompt weight is 1 if not specified.\n",
    "answers = stability_api.generate(\n",
    "    prompt= [generation.Prompt(text=\"a mountain landscape\",parameters=generation.PromptParameters(weight=1)), \n",
    "    generation.Prompt(text=\"in the style of thomas kinkade\",parameters=generation.PromptParameters(weight=1)), \n",
    "    generation.Prompt(text=\"tree\",parameters=generation.PromptParameters(weight=-1.3))], # Negative prompting is now possible via the API, simply assign a negative weight to a prompt.\n",
    "    # In the example above we are combining a mountain landscape with the style of thomas kinkade, and we are negative prompting trees out of the resulting concept.\n",
    "    # When determining prompt weights, the total possible range is [-10, 10] but we recommend staying within the range of [-2, 2].\n",
    "    seed=9, # If a seed is provided, the resulting generated image will be deterministic.\n",
    "                    # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                    # Note: This is only true for non-CLIP Guided generations. \n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30. \n",
    "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    samples=1, # Number of images to generate, defaults to 1 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, save generated images.\n",
    "for resp in answers:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            img = Image.open(io.BytesIO(artifact.binary))\n",
    "            img.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92013b8d",
   "metadata": {},
   "source": [
    "## VARIANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "adb104f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our initial generation parameters.\n",
    "answers = stability_api.generate(\n",
    "    prompt=\"The satellite hovers high above the Earth, its powerful cameras capturing a stunning view of our planet. The blue of the oceans contrasts with the green and brown of the land masses, creating a beautiful mosaic of colors.\",\n",
    "    seed=10, # If a seed is provided, the resulting generated image will be deterministic.\n",
    "                    # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                    # Note: This isn't quite the case for Clip Guided generations, which we'll tackle in a future example notebook.\n",
    "    steps=50, # Step Count defaults to 50 if not specified here.\n",
    "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    samples=1, # Number of images to generate, defaults to 1 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2S_ANCESTRAL # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, display generated image.\n",
    "for resp in answers:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            global img\n",
    "            img = Image.open(io.BytesIO(artifact.binary))\n",
    "            img.save(str(artifact.seed)+ \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da2415dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our initial generation parameters.\n",
    "answers2 = stability_api.generate(\n",
    "    prompt=\"waterfall\",\n",
    "    init_image=img, # Assign our previously generated img as our Initial Image for transformation.\n",
    "    start_schedule=0.6, # Set the strength of our prompt in relation to our initial image.\n",
    "    seed=11, # If attempting to transform an image that was previously generated with our API, initial images benefit from having their own distinct seed rather than using the seed of the original image generation.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30. \n",
    "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt. Setting this value higher increases the strength in which it tries to match your prompt. Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    samples=4, # Number of images to generate, defaults to 1 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2S_ANCESTRAL # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m)\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped. If adult content classifier is not tripped, display generated image.\n",
    "for resp in answers2:\n",
    "    for artifact in resp.artifacts:\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                \"Please modify the prompt and try again.\")\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            global img2\n",
    "            img2 = Image.open(io.BytesIO(artifact.binary)) # Set our resulting initial image generations as 'img2' to avoid overwriting our previous 'img' generation.\n",
    "            img2.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e90099",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The satellite hovers high above the Earth, its powerful cameras capturing a stunning view of our planet. The blue of the oceans contrasts with the green and brown of the land masses, creating a beautiful mosaic of colors.\",\n",
    "grassy with water flow : \"sun-drenched meadow, surrounded by tall trees and rolling hills. A clear stream winds its way through the grass, bubbling over rocks and creating small pools of crystal-clear water. Wildflowers bloom in a riot of color, adding a splash of brightness to the green grass.\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b88421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\neera\\anaconda3\\lib\\site-packages (8.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10256138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
